## Tokenizers from Scratch
This repository contains a Jupyter Notebook that explores the fundamental concept of tokenization in Large Language Models (LLMs). The notebook was created by following Andrej Karpathy's comprehensive video tutorial on building GPT's tokenizer from scratch.

## Overview
Tokenization is a critical step in natural language processing (NLP) that involves breaking down text into smaller units called tokens. These tokens are then used by language models to understand and generate human language. This notebook delves into the reasons why understanding tokenization from scratch is essential for grasping the inner workings and limitations of LLMs.

## Key Reasons to Learn Tokenization
Understanding tokenization provides answers to several intriguing questions about LLMs:

* Why can't LLMs spell words correctly? Tokenization.
* Why do LLMs struggle with simple string processing tasks like reversing a string? Tokenization.
* Why are LLMs less effective with non-English languages (e.g., Japanese)? Tokenization.
* Why do LLMs falter at simple arithmetic? Tokenization.
* Why did GPT-2 encounter unnecessary difficulties coding in Python? Tokenization.
* Why do LLMs abruptly halt when they see the string ""? Tokenization.
* What is the mysterious warning about "trailing whitespace"? Tokenization.
* Why do LLMs break when asked about "SolidGoldMagikarp"? Tokenization.
* Why should YAML be preferred over JSON with LLMs? Tokenization.
* Why are LLMs not truly end-to-end language models? Tokenization.
* What is the real root of suffering? Tokenization.

## Contents
Byte-Pair-Encoding_from_scratch.ipynb: The Jupyter Notebook that walks through the process of building a tokenizer from scratch, following Andrej Karpathy's tutorial.
https://github.com/karpathy/minbpe

## Acknowledgements
A huge thank you to Andrej Karpathy for his enlightening tutorial and continuous contributions to the field of AI.
